# 语义理解&深度学习

-   什么是语言理解

    ![image-20210623104400752](预训练语言模型及应用.assets/image-20210623104400752.png)

-   如何使用词/文本词向量
    -   ![image-20210623104619312](预训练语言模型及应用.assets/image-20210623104619312.png)

-   问题

    -   <img src="预训练语言模型及应用.assets/image-20210623104636454.png" alt="image-20210623104636454" style="zoom:50%;" />

-   解决方案：预训练+微调机制

    -   <img src="预训练语言模型及应用.assets/image-20210623104727686.png" alt="image-20210623104727686" style="zoom:50%;" />

    -   <img src="预训练语言模型及应用.assets/image-20210623104752626.png" alt="image-20210623104752626" style="zoom:50%;" />

    -   原因：

        <img src="预训练语言模型及应用.assets/image-20210623104825386.png" alt="image-20210623104825386" style="zoom:50%;" />

    -   迁移至NLP

        <img src="预训练语言模型及应用.assets/image-20210623104900140.png" alt="image-20210623104900140" style="zoom:50%;" />

    

    # 基于预训练的语义理解技术

    

    -   ELMo

        -   <img src="预训练语言模型及应用.assets/image-20210623104946049.png" alt="image-20210623104946049" style="zoom:50%;" />

        -   <img src="预训练语言模型及应用.assets/image-20210623105024612.png" alt="image-20210623105024612" style="zoom:50%;" />

        -   使用方法：feature-based

            -   $E = a_1E_1 + a_2E_2 + a_3E_3$, $a_1, a_2, a_3$: learnable -> ELMo 是动态可学习的词向量

        -   <img src="预训练语言模型及应用.assets/image-20210623105057545.png" alt="image-20210623105057545" style="zoom:50%;" />

        -   解决多义词问题

            <img src="预训练语言模型及应用.assets/image-20210623105311147.png" alt="image-20210623105311147" style="zoom:50%;" />

        -   问题：

            ![image-20210623105333097](预训练语言模型及应用.assets/image-20210623105333097.png)

    -   GPT：

        -   <img src="预训练语言模型及应用.assets/image-20210623105419102.png" alt="image-20210623105419102" style="zoom:50%;" />
        -   <img src="预训练语言模型及应用.assets/image-20210623105450501.png" alt="image-20210623105450501" style="zoom:50%;" />
        -   下游任务使用方式： Model-based
            -   <img src="预训练语言模型及应用.assets/image-20210623105537306.png" alt="image-20210623105537306" style="zoom:50%;" />
            -   <img src="预训练语言模型及应用.assets/image-20210623105618418.png" alt="image-20210623105618418" style="zoom:50%;" />
        -   缺点：
            -   <img src="预训练语言模型及应用.assets/image-20210623105637312.png" alt="image-20210623105637312" style="zoom:50%;" />

    -   BERT

        -   <img src="预训练语言模型及应用.assets/image-20210623105704574.png" alt="image-20210623105704574" style="zoom:50%;" />
        -   <img src="预训练语言模型及应用.assets/image-20210623105820289.png" alt="image-20210623105820289" style="zoom:50%;" />
        -   使用方式：实验 -> Model-based更优
            -   ![image-20210623105920754](预训练语言模型及应用.assets/image-20210623105920754.png)
            -   <img src="预训练语言模型及应用.assets/image-20210623110141788.png" alt="image-20210623110141788" style="zoom:50%;" />
        -   缺点：
            -   <img src="预训练语言模型及应用.assets/image-20210623105939828.png" alt="image-20210623105939828" style="zoom:50%;" />

    -   ERNIE

        -   改动： mask-level 从token扩展至词或者短语

        -   <img src="预训练语言模型及应用.assets/image-20210623110035390.png" alt="image-20210623110035390" style="zoom:50%;" />

        -   <img src="预训练语言模型及应用.assets/image-20210623110100707.png" alt="image-20210623110100707" style="zoom:50%;" />

        -   效果：

            ​	<img src="预训练语言模型及应用.assets/image-20210623110205439.png" alt="image-20210623110205439" style="zoom:50%;" />

        -   类似模型：
            -   19.5 Google **BERT-wwm**
            -   19.7 Facebook **SpanBERT**
        -   缺点：
            -   <img src="预训练语言模型及应用.assets/image-20210623110346726.png" alt="image-20210623110346726" style="zoom:50%;" />
        -   ERNIE2.0 持续学习
            -   <img src="预训练语言模型及应用.assets/image-20210623110417963.png" alt="image-20210623110417963" style="zoom:50%;" />
            -   <img src="预训练语言模型及应用.assets/image-20210623110501080.png" alt="image-20210623110501080" style="zoom:50%;" />
            -   <img src="预训练语言模型及应用.assets/image-20210623110521826.png" alt="image-20210623110521826" style="zoom:50%;" />
            -   <img src="预训练语言模型及应用.assets/image-20210623110538108.png" alt="image-20210623110538108" style="zoom:50%;" />

    -   总结：

        -   <img src="预训练语言模型及应用.assets/image-20210623110602335.png" alt="image-20210623110602335" style="zoom:50%;" />



# 预训练模型在NLP经典任务的应用

-   <img src="预训练语言模型及应用.assets/image-20210623110921847.png" alt="image-20210623110921847" style="zoom:50%;" />

-   <img src="预训练语言模型及应用.assets/image-20210623110939202.png" alt="image-20210623110939202" style="zoom:50%;" />

-   文本匹配

    -   <img src="预训练语言模型及应用.assets/image-20210623111025454.png" alt="image-20210623111025454" style="zoom:50%;" />
    -   <img src="预训练语言模型及应用.assets/image-20210623111056735.png" alt="image-20210623111056735" style="zoom:50%;" />

    -   两类： 双塔及单塔， 每个塔就是一个transformer encoder

        -   ​	<img src="预训练语言模型及应用.assets/image-20210623111152672.png" alt="image-20210623111152672" style="zoom:50%;" />

        -   <img src="预训练语言模型及应用.assets/image-20210623111237507.png" alt="image-20210623111237507" style="zoom:50%;" />
        -   <img src="预训练语言模型及应用.assets/image-20210623111341809.png" alt="image-20210623111341809" style="zoom:50%;" />

    

    # Final

    

    <img src="预训练语言模型及应用.assets/image-20210623111409234.png" alt="image-20210623111409234" style="zoom:50%;" />

