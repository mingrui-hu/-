# DARPA可解释AI研究（XAI计划）的4年回顾与经验总结

- https://mp.weixin.qq.com/s/EuNi1kfhkZvoR9mAPx6dfg
- **导语：**DARPA（美国防部高级研究计划局）于 2015 年制定了可解释人工智能 (XAI) 计划，目标是使最终用户能够更好地理解、信任和有效管理人工智能系统。**2017年，为期4年的XAI研究计划启动。现在，随着 XAI 在 2021 年结束，本文总结和反思了 XAI 项目的目标、组织和研究进展。**
- https://xaitk.org/getting-started/

## XAI Program Structure

### 技术领域TAs(Technical Areas)

- Explainable Learners:
  - Teams provide:
    - Explainable Model
    - Explanation Interface

### 评估框架

- 检测：
  - 训练过程
  - 模型表现
  - 解释界面

## XAI可解释学习方法

1. 可解释的模型方法

2. 深度解释方法

3. 模型归纳技术

- 11个XAI团队逐渐探索了一些机器学习方法，如可**操作的概率模型**和**因果模型**以及解释
  技术，如**强化学习算法产生的状态机**、**贝叶斯教学**、**视觉显著性地图**以及**网络和GAN剖析**。  

- **将机器学习和解释技术结合**起来，并通过精心设计的**心理学实验**来评估解释的有效性。  

## 总结

- **与只提供决策的系统相比，用户更喜欢提供决策与解释的系统**。在那些用户需要了解人工智能系统如
  何做出决定的内部运作的任务中，解释能够提供最大的价值。(由11个团队的实验支持)。
  为了使解释能够提高用户任务表现，任务必须足够困难，以便于人工智能的解释起作用（PARC，UT
  Dallas）。
- **用户的认知负荷会阻碍用户解读解释时的表现**。结合前面的观点，解释和任务难度需要被校准，以提
  高用户的表现（加州大学洛杉矶分校、俄勒冈州立大学）。
- 当人工智能**不正确时，解释更有帮助**，这一点对**边缘案例**特别有价值（加州大学洛杉矶分校、罗格斯
  大学）。
- 解释的有效性的措施可以随着时间的推移而改变（Raytheon, BBN）。
- 相对于单独的解释而言，**建议性（Advisability）**可以明显提高用户的信任度（加州大学伯克利分
  校）。
  - 加州大学伯克利分校的研究结果表明，**建议性**，即人工智能**系统接受用户建议**的能力，可以提高用户对解
    释以外的信任度。当然，用户可能更喜欢那些像人类一样可以**快速提供反馈，纠正行为的系统**。这种**既能**
    **生产又能消费解释**的可建议的人工智能系统将成为实现人类和AI系统之间更紧密合作的关键  
  - ==Q: 除了直接给一个标签，如何反馈解释？-> both for model agnostic & dependent explainer==
- XAI对于测量和调整用户和XAI系统的心理模型很有帮助（罗格斯、SRI）。
- 最后，由于XAI的最后一年发生在COVID-19大流行的时期，我们的执行团队开发了设计网络界面的
  最佳实践，以便在不可能进行现场研究时进行XAI用户研究（OSU、UCLA  