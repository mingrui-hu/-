### counterfactual what if tools

- https://pair-code.github.io/what-if-tool/

### Captum

### US DPARI

https://xaitk.org/getting-started/

[DALEX](https://dalex.drwhy.ai/)

[AIX360](http://aix360.mybluemix.net/)

### from aaai 2021 & 2022 xai tutorial p134/p136

● **QUANTUS**: https://github.com/understandable-machine-intelligence-lab/Quantus (with metrics)
● **DEEL XPLIQUE**: https://github.com/deel-ai/xplique (combination of existing tools: feature attribution + Open AI visualization + Google concept explanation) (with metrics)
● **Facebook Fairseq**: https://github.com/pytorch/fairseq (to capture attention weights per input token… and much more)
● Saliency-based XAI: https://github.com/chihkuanyeh/saliency_evaluation + https://github.com/pair-code/saliency/blob/master/Examples.ipynb (Vanilla Gradients, Guided Backpropogation, Integrated Gradients, Occlusion)
● Explainer: https://github.com/dbvis-ukon/explainer (explainable AI and interactive machine learning)
● XAI Empirical studies: https://paperswithcode.com/paper/how-can-i-explain-this-to-you-an-empirical
● Facebook Captum - https://github.com/pytorch/captum
● IBM-MIT shared-interest https://github.com/aboggust/shared-interest
● Google-CMU Post-training Concept-based Explanation: https://github.com/chihkuanyeh/concept_exp
● Google-Stanford Automatic Concept-based Explanations: https://github.com/amiratag/ACE
● Google Testing with Concept Activation Vectors https://github.com/tensorflow/tcav
● DeepExplain: perturbation and gradient-based attribution methods for Deep Neural Networks interpretability. github.com/marcoancona/DeepExplain
● iNNvestigate: A toolbox to iNNvestigate neural networks' predictions. github.com/albermax/innvestigate
● SHAP: SHapley Additive exPlanations. github.com/slundberg/shap
● Microsoft Explainable Boosting Machines. https://github.com/Microsoft/interpret
● **GANDissect: Pytorch-based tools for visualizing and understanding the neurons of a GAN**. https://github.com/CSAILVision/GANDissect
● ELI5: A library for debugging/inspecting machine learning classifiers and explaining their predictions. github.com/TeamHG-Memex/eli5
● Skater: Python Library for Model Interpretation/Explanations. github.com/datascienceinc/Skater
● Yellowbrick: Visual analysis and diagnostic tools to facilitate machine learning model selection. github.com/DistrictDataLabs/yellowbrick
● Lucid: A collection of infrastructure and tools for research in neural network interpretability. github.com/tensorflow/lucid
● LIME: Agnostic Model Explainer. https://github.com/marcotcr/lime
● Sklearn_explain: model individual score explanation for an already trained scikit-learn model. https://github.com/antoinecarme/sklearn_explain
● Heatmapping: Prediction decomposition in terms of contributions of individual input variables
● Deep Learning Investigator: Investigation of Saliency, Deconvnet, GuidedBackprop and more. https://github.com/albermax/innvestigate
● **Google PAIR What-if**: Model comparison, **counterfactual**, individual similarity. https://pair-code.github.io/what-if-tool/
● Google tf-explain: https://tf-explain.readthedocs.io/en/latest/
● **IBM AI Fairness: Set of fairness metrics** for datasets and ML models, explanations for these metrics. https://github.com/IBM/aif360
● Blackbox auditing: Auditing Black-box Models for Indirect Influence. https://github.com/algofairness/BlackBoxAuditing
● Model describer: Basic statiscal metrics for explanation (visualisation for error, sensitivity). https://github.com/DataScienceSquad/model-describer
● AXA Interpretability and Robustness: https://axa-rev-research.github.io/ (more on research resources – not much about tools)  
