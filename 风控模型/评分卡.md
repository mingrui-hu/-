# 评分卡

### 1. 适用客群

- 通用评分卡
- 定制评分卡
- 子评分卡

### 2. 模型设计

	- 特有: 数据集时间段设计
	- 定量化**是否还款**： 滚动率分析 + 账龄分析 + 时间窗口设计
 - 账龄分析：
    - vintage曲线 
       - 前几期逾期率上升很快，说明(现有模型)短期风险捕捉能力差， 可能有欺诈
       - 
    - 取大多数vintage曲线都趋于平稳的某特殊月份值作为表现期

### 3. 数据集切分

- 特殊： OOT集 而非 常用test集
  - 通常用观测点前最后一个月的样本作为oot

### 4. 数据

- 资质类数据
- 信贷类：多头 + 征信
- 授权信息
- 平台表现
- 埋点数据



### 用户分群

- 经验分群
  - 参考DTI(债务收入比)指标
- 技术分群
  - 决策树 -> 用于生成组合策略
  - 无监督GMM分群
  - 聚类
    - k-mean
    - mean-shift
    - 层次聚类
    - tsne
    - dbscan
  - 聚类后(簇内/全局)方差分析 -> 主要影响分类结果的特征

### 特征聚合

- 对多个时间节点(时间切片)的取值进行聚合
- 时序算子特征聚合 35+ (p127 5.2.1)

- 衍生函数类及多进程版本见代码 p139
- 具有较高共线性

### 特征组合

- 特征分段组合
- 决策树基于特定指标进行贪心搜索 ->最优特征组合

### 特征变换

- 分箱
  - 双变量图(Bivar), 纵轴：负样本占比
  - 箱号编码 -> WOE
  - 分箱方法：卡方， 



## 2021-09-24 score与prob的转换

#### 1. Score通用校准

1. 通用校准公式

$$
\begin{split}
score &= base\_score + pdo * odds \\
	  &= base\_score + pdo *log_2(P_{pos}/P_{neg})
\end{split}
$$

2. 逻辑回归特性
   $$
   odds = ln(P_{pos}/P_{neg}) = W^TX
   $$

   $$
   \begin{split}
   score &= base\_score + pdo *log_2(P_{pos}/P_{neg}) \\
   	&= base\_score + pdo * \frac{ln(P_{pos}/P_{neg})}{ln2} (换底) \\
   	&= base\_score + pdo * W^TX
   \end{split}
   $$

   

3. 标准评分卡模型做了分箱并使用lr， 

   **pro:** 

   a. 可以利用2， 在推理阶段直接通过记录lr的**bins2score** mapping 进行查表并在特征维度将$score_{bin_i}$相加。

   b. 部署时不需要任何模型文件，只需要一个字典mapping文件。

   [**代价**: 在训练阶段将所有可能出现的特征bin(最终的$x_i$)对应的partial score($score_{bin_i} = w_i * x_i = w_i * bin_i$) 都计算出来并直接保存为规则(rules), 这样可以在推理时绕过一层*bin2woe*的转换:
   
   ```python
       def _generate_rules(self):
           if not self._check_rules(self.combiner, self.transer):
               raise Exception('generate failed')
           
           rules = {}
   
           for idx, key in enumerate(self.features_):
               weight = self.coef_[idx]
   
               if weight == 0:
                   continue
               
               woe = self.transer[key]['woe']
               
               rules[key] = {
                   'bins': self.combiner[key],
                   'woes': woe,
                   'weight': weight,
                   'scores': self.woe_to_score(woe, weight = weight),
               }
           
           return rules
   ```
   
   **bin $\rightarrow$ woe $\rightarrow$ woe_score mapping_dict** 简化为
   
   **bin $\rightarrow$ bin_score mapping_dict**]
   
   ```python
       def bin_to_score(self, bins, return_sub = False):
           """predict score from bins
           """
           res = bins.copy()
           for col in self.rules:
               s_map = self.rules[col]['scores']
               b = bins[col].values
               # set default group to min score
               b[b == self.EMPTY_BIN] = np.argmin(s_map)
               # replace score
               res[col] = s_map[b]
   
           score = np.sum(res.values, axis = 1)
   
           if return_sub is False:
               return score
   
           return score, res
   ```
   
   

#### 2. 不考虑评分转换含义的映射方式

$score = base\_score + pdo * log_2(pred - lag)$

此时: 

- $pred$ 即模型输出的似然概率(i.e. $prob_{pos}$)
- $lag$ 通常设置为期望模型score等于基础分时的概率(i.e. $log_2(pred - lag) = 0$)
  - 假设期望模型的20%分位点作为基础分， 则只需将模型在测试集上排在20%分位的样本的概率值作为lag即可(from 智能风控黑皮书8.2.1)

#### 3. **prob $\leftrightarrow$ score** (通用校准方式)

- 与是否使用lr无关， 至于$prob$, $pdo(factor)$, $base\_score(offset)$有关
  $$
  \begin{split}
  & pdo = Factor ∗ ln (2) \\
  
  & Factor = pdo / ln (2) \\
  
  & offset = score - Factor ∗ ln (odds)
  \end{split}
  $$
  

```python
def proba_to_score(self, prob):
    """covert probability to score

        odds = (1 - prob) / prob
        score = factor * log(odds) + offset
        """
    return self.factor * (np.log(1 - prob) - np.log(prob)) + self.offset


def score_to_proba(self, score):
    """covert score to probability

        Returns:
            array-like|float: the probability of `1`
        """
        return 1 / (np.e ** ((score - self.offset) / self.factor) + 1)

```



## By 2021-09-24 LEFT PROBLEMS

- 共线性 及 vif
- stepwise selection & F检验
  - [toad stepwise explanation](https://toad.readthedocs.io/en/latest/reference.html#scorecard-transformation)
  - AIC/BIC
- KENDALL correlation
- PSI & KS 公式解析
- WOE 贝叶斯角度， i.e. why is `evidence`
- feature importance filter
